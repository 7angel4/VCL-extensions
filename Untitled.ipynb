{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3950908-d649-4250-b01f-ae828ad30b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808f0a1e-b78c-462d-aa57-a74974d44956",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "MNIST_INPUT_DIM = 784\n",
    "MNIST_IMG_SIZE = int(np.sqrt(MNIST_INPUT_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944be05a-e970-4803-8f31-d4504c4b20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for the VCL experiment\"\"\"\n",
    "    def __init__(self):\n",
    "        # Model hyperparameters\n",
    "        self.prior_type = 'gaussian'  # 'gaussian' or 'exponential'\n",
    "        self.task_type = 'classification'\n",
    "        self.init_prior_mu = 0.0\n",
    "        self.init_prior_scale = 0.01\n",
    "        self.input_dim = 784\n",
    "        self.hidden_dim = 256\n",
    "        self.num_samples = 10\n",
    "        \n",
    "        # Training parameters\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 256\n",
    "        self.learning_rate = 0.001\n",
    "        self.coreset_method = \"random\"\n",
    "        self.coreset_size = 200\n",
    "        self.patience = 5\n",
    "        self.early_stop_threshold = 1e-4\n",
    "        \n",
    "        # Task configuration\n",
    "        self.tasks = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "        \n",
    "    def validate(self):\n",
    "        assert self.prior_type in ['gaussian', 'exponential']\n",
    "        assert len(self.tasks) > 0\n",
    "\n",
    "    def get_coreset_fn(self):\n",
    "        \"\"\"Get coreset attachment function based on config\"\"\"\n",
    "        if self.coreset_method == \"kcenter\":\n",
    "            return attach_kcenter_coreset\n",
    "        return attach_random_coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0ab9d45-53e2-4d45-aec5-040cdace6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_gaussian(q, p):\n",
    "    \"\"\"KL divergence between two Gaussian distributions\"\"\"\n",
    "    q_mu, q_sigma = q['mu'], q['sigma']\n",
    "    p_mu, p_sigma = p['mu'], p['sigma']\n",
    "    \n",
    "    ratio = (q_sigma / p_sigma) ** 2\n",
    "    log_ratio = torch.log(ratio)\n",
    "    mean_term = ratio + ((q_mu - p_mu) / p_sigma) ** 2\n",
    "    return 0.5 * (log_ratio + mean_term - 1)\n",
    "\n",
    "def kl_exponential(q, p):\n",
    "    \"\"\"KL divergence between two Exponential distributions\"\"\"\n",
    "    q_lambda = 1 / q['sigma']  # Convert scale to rate\n",
    "    p_lambda = 1 / p['sigma']\n",
    "    \n",
    "    log_ratio = torch.log(p_lambda / q_lambda)\n",
    "    ratio_term = q_lambda / p_lambda\n",
    "    return log_ratio + ratio_term - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4775d7-bb08-4403-a7a3-9b0f48b3a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalLayer(nn.Module):\n",
    "    \"\"\"Variational continual learning layer with configurable prior\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.config = config\n",
    "        \n",
    "        # Weight parameters\n",
    "        self.W_mu = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.W_rho = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        \n",
    "        # Bias parameters\n",
    "        self.b_mu = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.b_rho = nn.Parameter(torch.Tensor(output_dim))\n",
    "        \n",
    "        # Initialize priors\n",
    "        self.W_prior = {\n",
    "            'mu': torch.tensor(config.init_prior_mu),\n",
    "            'sigma': torch.tensor(config.init_prior_scale)\n",
    "        }\n",
    "        self.b_prior = {\n",
    "            'mu': torch.tensor(config.init_prior_mu),\n",
    "            'sigma': torch.tensor(config.init_prior_scale)\n",
    "        }\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # Initialize means\n",
    "        nn.init.normal_(self.W_mu, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.b_mu, mean=0.0, std=0.1)\n",
    "        \n",
    "        # Initialize rho for variance\n",
    "        self.W_rho.data.fill_(-3.0)\n",
    "        self.b_rho.data.fill_(-3.0)\n",
    "    \n",
    "    @property\n",
    "    def W_sigma(self):\n",
    "        \"\"\"Convert rho to sigma using softplus\"\"\"\n",
    "        return torch.log1p(torch.exp(self.W_rho))\n",
    "    \n",
    "    @property\n",
    "    def b_sigma(self):\n",
    "        \"\"\"Convert rho to sigma using softplus\"\"\"\n",
    "        return torch.log1p(torch.exp(self.b_rho))\n",
    "    \n",
    "    def forward(self, x, sample=True):\n",
    "        \"\"\"Forward pass with local reparameterization trick\"\"\"\n",
    "        # Calculate activations\n",
    "        act_mu = F.linear(x, self.W_mu, self.b_mu)\n",
    "        \n",
    "        if self.training or sample:\n",
    "            act_var = F.linear(x**2, self.W_sigma**2, self.b_sigma**2)\n",
    "            act_std = torch.sqrt(act_var + 1e-16)\n",
    "            noise = torch.randn_like(act_mu)\n",
    "            return act_mu + act_std * noise\n",
    "        return act_mu\n",
    "    \n",
    "    def kl_loss(self):\n",
    "        \"\"\"Compute KL divergence based on prior type\"\"\"\n",
    "        W_params = {'mu': self.W_mu, 'sigma': self.W_sigma}\n",
    "        b_params = {'mu': self.b_mu, 'sigma': self.b_sigma}\n",
    "        \n",
    "        if self.config.prior_type == 'exponential':\n",
    "            kl_func = kl_exponential\n",
    "        else:\n",
    "            kl_func = kl_gaussian\n",
    "            \n",
    "        return (torch.sum(kl_func(W_params, self.W_prior))) + \\\n",
    "               (torch.sum(kl_func(b_params, self.b_prior)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa71566-febf-4984-9102-9736746e605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \"\"\"Base model for SplitMNIST experiments\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tasks = config.tasks\n",
    "        \n",
    "    def forward(self, x, task_id):\n",
    "        x = x.view(-1, self.config.input_dim)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.task_heads[task_id](x)\n",
    "\n",
    "class VanillaModel(BaseModel):\n",
    "    \"\"\"Standard neural network without VCL\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(config.input_dim, config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        ])\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_dim, len(task)) for task in self.tasks\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "177a2571-ba80-4a52-88b6-b358662abef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCLModel(BaseModel):\n",
    "    \"\"\"Variational Continual Learning model\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            VariationalLayer(config.input_dim, config.hidden_dim, config),\n",
    "            VariationalLayer(config.hidden_dim, config.hidden_dim, config)\n",
    "        ])\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            VariationalLayer(config.hidden_dim, len(task), config) for task in self.tasks\n",
    "        ])\n",
    "    \n",
    "    def update_priors(self):\n",
    "        \"\"\"Update priors to current posteriors after learning a task\"\"\"\n",
    "        for layer in self.hidden_layers + list(self.task_heads):\n",
    "            layer.W_prior = {\n",
    "                'mu': layer.W_mu.detach().clone(),\n",
    "                'sigma': layer.W_sigma.detach().clone()\n",
    "            }\n",
    "            layer.b_prior = {\n",
    "                'mu': layer.b_mu.detach().clone(),\n",
    "                'sigma': layer.b_sigma.detach().clone()\n",
    "            }\n",
    "    \n",
    "    def total_kl_loss(self, task_id):\n",
    "        \"\"\"Compute total KL loss for current task\"\"\"\n",
    "        kl_loss = 0.0\n",
    "        for layer in self.hidden_layers:\n",
    "            kl_loss += layer.kl_loss()\n",
    "        kl_loss += self.task_heads[task_id].kl_loss()\n",
    "        return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79f72f5f-70cd-4b7b-ac76-308f90dafc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(outputs, targets, config):\n",
    "    if config.task_type == 'regression': # Gaussian likelihood - MSE loss\n",
    "        loss = F.mse_loss(outputs.mean(-1), targets)\n",
    "    else: # Categorical likelihood - NLL loss\n",
    "        log_output = torch.logsumexp(outputs, dim=-1) - np.log(config.num_samples)\n",
    "        loss = F.nll_loss(log_output, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42137bd4-e30f-411c-9136-5b9061c4ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, task_id, config):\n",
    "    \"\"\"Train model on a specific task\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    model.train()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    num_consec_worse_epochs = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            targets -= model.tasks[task_id][0]  # Reindex targets\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Monte Carlo sampling\n",
    "            outputs = torch.zeros(inputs.size(0), len(model.tasks[task_id]), \n",
    "                                config.num_samples, device=DEVICE)\n",
    "            for i in range(config.num_samples):\n",
    "                net_out = model(inputs, task_id)\n",
    "                # if config.task_type == 'classification':\n",
    "                #     net_out = F.log_softmax(net_out, dim=-1)  # Only for classification\n",
    "                outputs[..., i] = net_out\n",
    "\n",
    "            loss = compute_loss(outputs, targets, config)\n",
    "\n",
    "            # Add KL loss for VCL\n",
    "            if isinstance(model, VCLModel):\n",
    "                num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                loss += model.total_kl_loss(task_id) / num_params\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss - config.early_stop_threshold:\n",
    "            best_loss = epoch_loss\n",
    "            num_consec_worse_epochs = 0\n",
    "        else:\n",
    "            num_consec_worse_epochs += 1\n",
    "            if num_consec_worse_epochs >= config.patience:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4577298-0899-4846-96dc-741f58651661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, task_id, config):\n",
    "    \"\"\"Evaluate model supporting both classification and regression\"\"\"\n",
    "    model.eval()\n",
    "    metrics = []\n",
    "    task = model.tasks[task_id]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            # Prepare targets based on task type\n",
    "            if config.task_type == 'regression':\n",
    "                targets = F.one_hot(targets, num_classes=10).float()\n",
    "            else:\n",
    "                targets -= task[0]\n",
    "            \n",
    "            outputs = torch.zeros(inputs.size(0), len(task), \n",
    "                                config.num_samples, device=DEVICE)\n",
    "            for i in range(config.num_samples):\n",
    "                net_out = model(inputs, task_id)\n",
    "                outputs[..., i] = net_out\n",
    "            \n",
    "            # Calculate metric based on task type\n",
    "            if config.task_type == 'regression':\n",
    "                pred = outputs.mean(-1)\n",
    "                rmse = torch.sqrt(F.mse_loss(pred, targets))\n",
    "                metrics.append(rmse.item())\n",
    "            else:\n",
    "                log_output = torch.logsumexp(outputs, dim=-1) - np.log(config.num_samples)\n",
    "                acc = (log_output.argmax(-1) == targets).float().mean()\n",
    "                metrics.append(acc.item())\n",
    "    \n",
    "    if config.task_type == 'regression':\n",
    "        return np.mean(metrics)  # Return average RMSE\n",
    "    else:\n",
    "        return np.mean(metrics)  # Return average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca5e16a-d660-4aa8-9687-d9858cdfd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "def get_class_indices(dataset, target_classes):\n",
    "    \"\"\"Get indices for specified class targets\"\"\"\n",
    "    idx = torch.zeros_like(dataset.targets, dtype=torch.bool)\n",
    "    for target in target_classes:\n",
    "        idx |= (dataset.targets == target)\n",
    "    return idx\n",
    "\n",
    "def create_split_dataloaders(class_distribution, batch_size=256):\n",
    "    \"\"\"Create train/test dataloaders for each task\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((MNIST_IMG_SIZE, MNIST_IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Load MNIST datasets\n",
    "    train_set = torchvision.datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "    dataloaders = []\n",
    "    \n",
    "    for classes in class_distribution:\n",
    "        # Train loader\n",
    "        train_idx = torch.where(get_class_indices(train_set, classes))[0]\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=batch_size,\n",
    "            sampler=SubsetRandomSampler(train_idx)\n",
    "        )\n",
    "        \n",
    "        # Test loader\n",
    "        test_idx = torch.where(get_class_indices(test_set, classes))[0]\n",
    "        test_loader = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=batch_size,\n",
    "            sampler=SubsetRandomSampler(test_idx)\n",
    "        )\n",
    "        \n",
    "        dataloaders.append((train_loader, test_loader))\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d02990-d2b0-46dd-96d2-e3b6ad908b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2de6f595-5d02-44fe-9385-28387a7e7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config):\n",
    "    \"\"\"Run complete continual learning experiment\"\"\"\n",
    "    # Prepare data\n",
    "    dataloaders = create_split_dataloaders(config.tasks, config.batch_size)\n",
    "    \n",
    "    # Initialize models\n",
    "    vanilla_model = VanillaModel(config).to(DEVICE)\n",
    "    vcl_model = VCLModel(config).to(DEVICE)\n",
    "    \n",
    "    # Storage for results\n",
    "    results = {\n",
    "        'vanilla': np.zeros((len(config.tasks), len(config.tasks))),\n",
    "        'vcl': np.zeros((len(config.tasks), len(config.tasks)))\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate\n",
    "    for task_id in range(len(config.tasks)):\n",
    "        print(f\"\\n*** Training on Task {task_id+1} ***\")\n",
    "        \n",
    "        # Train vanilla model\n",
    "        train_model(vanilla_model, dataloaders[task_id][0], task_id, config)\n",
    "        \n",
    "        # Train VCL model\n",
    "        train_model(vcl_model, dataloaders[task_id][0], task_id, config)\n",
    "        if isinstance(vcl_model, VCLModel):\n",
    "            vcl_model.update_priors()\n",
    "        \n",
    "        # Evaluate on all tasks\n",
    "        for eval_task in range(task_id + 1):\n",
    "            _, test_loader = dataloaders[eval_task]\n",
    "            \n",
    "            # Vanilla evaluation\n",
    "            acc = evaluate(vanilla_model, test_loader, eval_task, config)\n",
    "            results['vanilla'][task_id, eval_task] = acc\n",
    "            \n",
    "            # VCL evaluation\n",
    "            acc = evaluate(vcl_model, test_loader, eval_task, config)\n",
    "            results['vcl'][task_id, eval_task] = acc\n",
    "            \n",
    "            print(f\"Task {eval_task+1} Accuracy - Vanilla: {acc:.4f}, VCL: {acc:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"Plot comparison of results\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Average accuracy plot\n",
    "    for model in results:\n",
    "        avg_acc = np.mean(results[model], axis=1)\n",
    "        ax1.plot(np.arange(len(avg_acc)) + 1, avg_acc, label=model)\n",
    "    ax1.set_title(\"Average Accuracy\")\n",
    "    ax1.set_xlabel(\"Number of Tasks\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Final accuracy plot\n",
    "    for model in results:\n",
    "        final_acc = results[model][-1]\n",
    "        ax2.plot(np.arange(len(final_acc)) + 1, final_acc, label=model)\n",
    "    ax2.set_title(\"Final Accuracy\")\n",
    "    ax2.set_xlabel(\"Task Number\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2572a9-3c87-41b9-913e-1ade6d5fd44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Training on Task 1 ***\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = ExperimentConfig()\n",
    "    config.prior_type = 'gaussian'  # Change to 'gaussian' for original VCL\n",
    "    config.validate()\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Run experiment\n",
    "    results = run_experiment(config)\n",
    "    plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c12a29-f8dd-4ce8-8399-e27353404760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce8ff2-cb49-4293-b581-d28f841af429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae196cb-d04c-4580-8ccd-48d7e526dd42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
