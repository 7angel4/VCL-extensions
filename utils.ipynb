{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "21c4630a-ce95-476b-9056-5776a8825515",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cc6e16e-95f0-42ba-9a12-d483963bfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_dir(config):\n",
    "    results_dir = RESULTS_DIR\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    return results_dir\n",
    "\n",
    "def plot_filepath(task_type, suffix=None):\n",
    "    return f\"{get_results_dir(config)}/{config.task_type}_{suffix}_plot.png\"\n",
    "\n",
    "def data_filepath(task_type, suffix=None):\n",
    "    return f\"{get_results_dir(config)}/{config.task_type}_{suffix}_data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "464bb178-2bb9-4c7c-aa65-8cf958842214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def export_results(data, task_type, datatype='mean'):\n",
    "    \"\"\" Exports the dictionary `data` to an HDF5 file. \"\"\"\n",
    "    with h5py.File(data_filepath(task_type, suffix=datatype), 'w') as f:\n",
    "        # Iterate over the dictionary and save each array as a dataset\n",
    "        for key, array in data.items():\n",
    "            f.create_dataset(key, data=array)\n",
    "\n",
    "def import_results(task_type, datatype='mean'):\n",
    "    \"\"\" Imports the data dictionary from an HDF5 file. \"\"\"\n",
    "    imported_data = {}\n",
    "    with h5py.File(data_filepath(task_type, suffix=datatype), 'r') as f:\n",
    "        # Iterate through the keys in the file and load the datasets\n",
    "        for key in f.keys():\n",
    "            imported_data[key] = f[key][:]\n",
    "    return imported_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fccdcfd5-d954-4786-aaea-d93c2c1324db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot styling based on model name\n",
    "def get_markerstyle(m):\n",
    "    if m.startswith('Gaussian'):\n",
    "        return 'o'\n",
    "    elif m.startswith('ExpGaussian'):\n",
    "        return 'v'\n",
    "    elif m.startswith('Exp'):\n",
    "        return 's'\n",
    "    else:\n",
    "        return 'p'\n",
    "\n",
    "def get_linestyle(m):\n",
    "    if m == VANILLA_MODEL:\n",
    "        return 'dashed'\n",
    "    return 'solid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3df17526-750d-4707-aa36-471ff993f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(config, results, results_std=None, export=True, title='Mean'):\n",
    "    \"\"\"Plot comparison of results with optional error bars.\"\"\"\n",
    "    MARKER_SIZE = 8\n",
    "    CAPSIZE = 4\n",
    "    ELINEWIDTH = 1\n",
    "    fig, ax = plt.subplots(figsize=(18, 6))\n",
    "    x_ticks = range(1, len(config.tasks)+1)\n",
    "    model_names = sorted(results.keys(), reverse=True)\n",
    "    \n",
    "    for model in model_names:\n",
    "        x_vals = np.arange(len(results[model])) + 1\n",
    "        # If std info is provided, add error bars\n",
    "        if results_std is not None:\n",
    "            ax.errorbar(x_vals, results[model], yerr=results_std[model], \n",
    "                         label=model, marker=get_markerstyle(model), linestyle=get_linestyle(model),\n",
    "                         markersize=MARKER_SIZE, capsize=CAPSIZE, elinewidth=ELINEWIDTH)\n",
    "        else:\n",
    "            ax.plot(x_vals, results[model], \n",
    "                     label=model, marker='o', markersize=MARKER_SIZE)\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_title(f\"{title} {config.eval_metric}\", fontsize=18)\n",
    "    ax.set_xlabel(\"# Tasks\", fontsize=15)\n",
    "    ax.legend(fontsize=15, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if export:\n",
    "        plt.savefig(plot_filepath(config, suffix=title.lower()))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5412e744-eee1-45fa-968d-cafb4730ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_results(config, results, results_std=None, export=True, mname_filter=None):\n",
    "    get_mean_val = (lambda res: np.mean(res, axis=0, where=(res > 1e-6)))\n",
    "    \n",
    "    mean_results = {m: get_mean_val(res) for m, res in results.items()}\n",
    "    mean_results_std = {m: get_mean_val(s) for m, s in results_std.items()}\n",
    "    if mname_filter is not None:\n",
    "        mean_results = {m: res for m, res in mean_results.items() if mname_filter(m)}\n",
    "        mean_results_std = {m: res for m, res in mean_results_std.items() if mname_filter(m)}\n",
    "    return plot_results(config, mean_results, results_std=mean_results_std, export=export, title='Mean')\n",
    "\n",
    "def plot_final_results(config, results, results_std=None, export=True, mname_filter=None):\n",
    "    final_results = {m: res[-1] for m, res in results.items()}\n",
    "    final_results_std = {m: s[-1] for m, s in results_std.items()}\n",
    "    if mname_filter is not None:\n",
    "        final_results = {m: res for m, res in final_results.items() if mname_filter(m)}\n",
    "        final_results_std = {m: res for m, res in final_results_std.items() if mname_filter(m)}\n",
    "    return plot_results(config, final_results, results_std=final_results_std, export=export, title='Final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f38383e4-43de-42c0-b62a-3602906fd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_if(msg, print_progress):\n",
    "    if print_progress:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c68a12d0-afaa-4dd1-a1c7-2e56d2ebb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_all_configs(task_type, config_filter=None, \n",
    "                    init_prior_scale=0.1, coreset_sizes = [100, 200]):\n",
    "\n",
    "    configs = []\n",
    "    combos = list(product(['gaussian', 'exponential'], [None, 'random', 'kcenter'], \n",
    "                          coreset_sizes, [True, False]))\n",
    "    # no coreset\n",
    "    combos = [(p,ca,0,u) if ca is None else (p,ca,cs,u) for p,ca,cs,u in combos]\n",
    "    # gaussian not affected by the update_prior_type attribute\n",
    "    combos = [(p,ca,cs,False) if p == 'gaussian' else (p,ca,cs,u) for p,ca,cs,u in combos]\n",
    "    combos = list(set(combos))\n",
    "    for combo in combos:\n",
    "        prior_type, coreset_alg_name, coreset_size, update_prior_type = combo\n",
    "        configs.append(ExperimentConfig(task_type=task_type,\n",
    "                                          prior_type=prior_type,\n",
    "                                          init_prior_scale=init_prior_scale,\n",
    "                                          coreset_alg_name=coreset_alg_name,\n",
    "                                          coreset_size=coreset_size,\n",
    "                                          update_prior_type=update_prior_type))\n",
    "    return list(filter(config_filter, configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45e2df-6e78-4c59-9eaa-d69537316987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
