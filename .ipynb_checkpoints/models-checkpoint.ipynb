{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14782003-ba13-4f98-b705-f440bf17bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.ipynb\n",
    "%run prior.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac97adcd-1e65-4066-98fd-7cb36b44a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalLayer(nn.Module):\n",
    "    \"\"\"Variational continual learning layer with configurable prior\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.config = config\n",
    "        \n",
    "        # Weight parameters\n",
    "        self.W_mu = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.W_rho = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        \n",
    "        # Bias parameters\n",
    "        self.b_mu = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.b_rho = nn.Parameter(torch.Tensor(output_dim))\n",
    "        \n",
    "        # Initialize priors (either gaussian or exponential)\n",
    "        self.W_prior = {\n",
    "            'mu': torch.tensor(config.init_prior_mu),\n",
    "            'sigma': torch.tensor(config.init_prior_scale)\n",
    "        }\n",
    "        self.b_prior = {\n",
    "            'mu': torch.tensor(config.init_prior_mu),\n",
    "            'sigma': torch.tensor(config.init_prior_scale)\n",
    "        }\n",
    "        \n",
    "        config.prior.init_params_for(self, init_mu=config.init_prior_mu, \n",
    "                                     init_scale=config.init_prior_scale, \n",
    "                                     init_const=config.init_const)\n",
    "        \n",
    "    @property\n",
    "    def W_sigma(self):\n",
    "        \"\"\"Convert rho to sigma using softplus\"\"\"\n",
    "        return torch.log1p(torch.exp(self.W_rho))\n",
    "    \n",
    "    @property\n",
    "    def b_sigma(self):\n",
    "        \"\"\"Convert rho to sigma using softplus\"\"\"\n",
    "        return torch.log1p(torch.exp(self.b_rho))\n",
    "    \n",
    "    def forward(self, x, sample=True):\n",
    "        \"\"\"Forward pass with local reparameterization trick\"\"\"\n",
    "        act_mu = F.linear(x, self.W_mu, self.b_mu)\n",
    "        if self.training or sample:\n",
    "            act_var = F.linear(x**2, self.W_sigma**2, self.b_sigma**2)\n",
    "            act_std = torch.sqrt(act_var + EPS_OFFSET)\n",
    "            noise = torch.randn_like(act_mu)\n",
    "            return act_mu + act_std * noise\n",
    "        return act_mu\n",
    "    \n",
    "    def kl_loss(self):\n",
    "        \"\"\"Compute KL divergence based on prior type\"\"\"\n",
    "        W_params = {'mu': self.W_mu, 'sigma': self.W_sigma}\n",
    "        b_params = {'mu': self.b_mu, 'sigma': self.b_sigma}\n",
    "        return (torch.sum(self.config.prior.kl(W_params, self.W_prior))) + \\\n",
    "               (torch.sum(self.config.prior.kl(b_params, self.b_prior)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43f12d7-2e4f-4a08-b67a-9bc149c963cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNN(nn.Module):\n",
    "    \"\"\"Base model for SplitMNIST experiments\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def task_id(self, t):\n",
    "        return 0 if len(self.task_heads) == 1 else t\n",
    "        \n",
    "    def forward(self, x, task_id):\n",
    "        x = x.view(-1, self.config.input_dim)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        # single-headed for PermutedMNIST\n",
    "        head_id = self.task_id(task_id)\n",
    "        return self.task_heads[head_id](x)\n",
    "\n",
    "    # def compute_loss(self, outputs, targets, task_id):  # dummy task_id arg for VCLNN's overriding method\n",
    "    #     print('compute_loss targets', targets)\n",
    "    #     if config.task_type == 'regression': # Gaussian likelihood - MSE loss\n",
    "    #         loss = F.mse_loss(outputs.mean(-1), targets)\n",
    "    #     else: # Categorical likelihood - negative log likelihood loss\n",
    "    #         log_output = torch.logsumexp(outputs, dim=-1) - np.log(self.config.num_samples)\n",
    "    #         loss = F.nll_loss(log_output, targets)\n",
    "    #     return loss\n",
    "    def compute_loss(self, outputs, targets, task_id):\n",
    "        return self.config.loss_fn(outputs, targets, task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b29cbb3-2461-473d-b4d8-40e537cc58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNN(BaseNN):\n",
    "    \"\"\"Standard neural network without VCL\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(config.input_dim, config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        ])\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_dim, d) for d in self.config.output_dims\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92468779-74ba-447d-875a-a65aa331195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCLNN(BaseNN):\n",
    "    \"\"\"Variational Continual Learning model\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            VariationalLayer(config.input_dim, config.hidden_dim, config),\n",
    "            VariationalLayer(config.hidden_dim, config.hidden_dim, config)\n",
    "        ])\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            VariationalLayer(config.hidden_dim, d, config) for d in self.config.output_dims\n",
    "        ])\n",
    "    \n",
    "    def update_priors(self):\n",
    "        \"\"\"Update priors to current posteriors after learning a task\"\"\"\n",
    "        for layer in self.hidden_layers + list(self.task_heads):\n",
    "            layer.W_prior = {\n",
    "                'mu': layer.W_mu.detach().clone(),\n",
    "                'sigma': layer.W_sigma.detach().clone()\n",
    "            }\n",
    "            layer.b_prior = {\n",
    "                'mu': layer.b_mu.detach().clone(),\n",
    "                'sigma': layer.b_sigma.detach().clone()\n",
    "            }\n",
    "    \n",
    "    def _kl_loss(self, task_id):\n",
    "        \"\"\"Compute KL loss for current task\"\"\"\n",
    "        kl_loss = 0.0\n",
    "        for layer in self.hidden_layers:\n",
    "            kl_loss += layer.kl_loss()\n",
    "        kl_loss += self.task_heads[task_id].kl_loss()\n",
    "        return kl_loss\n",
    "\n",
    "    # overrides BaseNN's compute_loss\n",
    "    def compute_loss(self, outputs, targets, task_id):\n",
    "        head_id = self.task_id(task_id)\n",
    "        loss = super(VCLNN, self).compute_loss(outputs, targets, head_id)\n",
    "        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        loss += self._kl_loss(head_id) / num_params\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9d9fb-dd5d-4b2c-bfa4-d5ad5329342d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84067a5-5a26-4b06-8c0f-aeea4a67f623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
